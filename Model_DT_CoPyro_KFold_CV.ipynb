{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "23253deaa44fc8808375d0438997a72d943f96b17c9c832de56565a7ac5f0797"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     C% Biomass  H% Biomass  O% Biomass  N% Biomass  C% HDPE  H% HDPE  \\\n",
       "0         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "1         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "2         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "3         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "4         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "..          ...         ...         ...         ...      ...      ...   \n",
       "242       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "243       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "244       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "245       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "246       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "\n",
       "     O% HDPE  N% HDPE  C% PS  H% PS  O% PS  N% PS  Time  Rate  Ratio  Temp  \\\n",
       "0          0      0.0    0.0    0.0    0.0      0   0.5     5   66.7   500   \n",
       "1          0      0.0    0.0    0.0    0.0      0   0.5     5  100.0   500   \n",
       "2          0      0.0    0.0    0.0    0.0      0   0.5     5   50.0   500   \n",
       "3          0      0.0    0.0    0.0    0.0      0   0.5     5   33.3   500   \n",
       "4          0      0.0    0.0    0.0    0.0      0   0.5     5    0.0   500   \n",
       "..       ...      ...    ...    ...    ...    ...   ...   ...    ...   ...   \n",
       "242        0      0.0   91.0    8.8    0.3      0  30.0   100   60.0   550   \n",
       "243        0      0.0   91.0    8.8    0.3      0  30.0   100   61.5   550   \n",
       "244        0      0.0   91.0    8.8    0.3      0  30.0   100   63.0   550   \n",
       "245        0      0.0   91.0    8.8    0.3      0  30.0   100   64.5   550   \n",
       "246        0      0.0   91.0    8.8    0.3      0  30.0   100   66.0   550   \n",
       "\n",
       "          Oil%      Char%       Gas%  \n",
       "0    60.156065  10.936392  28.907543  \n",
       "1    56.000000   0.000000  44.000000  \n",
       "2    45.993999  17.313035  36.692966  \n",
       "3    44.038082  23.267154  32.694764  \n",
       "4    25.762381  34.435228  39.802391  \n",
       "..         ...        ...        ...  \n",
       "242  16.633680  15.340660  13.907280  \n",
       "243  15.999231  14.779844  13.749581  \n",
       "244  15.349117  14.221099  13.605475  \n",
       "245  14.683338  13.664426  13.474963  \n",
       "246  14.001893  13.109825  13.358045  \n",
       "\n",
       "[247 rows x 19 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C% Biomass</th>\n      <th>H% Biomass</th>\n      <th>O% Biomass</th>\n      <th>N% Biomass</th>\n      <th>C% HDPE</th>\n      <th>H% HDPE</th>\n      <th>O% HDPE</th>\n      <th>N% HDPE</th>\n      <th>C% PS</th>\n      <th>H% PS</th>\n      <th>O% PS</th>\n      <th>N% PS</th>\n      <th>Time</th>\n      <th>Rate</th>\n      <th>Ratio</th>\n      <th>Temp</th>\n      <th>Oil%</th>\n      <th>Char%</th>\n      <th>Gas%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>66.7</td>\n      <td>500</td>\n      <td>60.156065</td>\n      <td>10.936392</td>\n      <td>28.907543</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>100.0</td>\n      <td>500</td>\n      <td>56.000000</td>\n      <td>0.000000</td>\n      <td>44.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>50.0</td>\n      <td>500</td>\n      <td>45.993999</td>\n      <td>17.313035</td>\n      <td>36.692966</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>33.3</td>\n      <td>500</td>\n      <td>44.038082</td>\n      <td>23.267154</td>\n      <td>32.694764</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>500</td>\n      <td>25.762381</td>\n      <td>34.435228</td>\n      <td>39.802391</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>242</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>60.0</td>\n      <td>550</td>\n      <td>16.633680</td>\n      <td>15.340660</td>\n      <td>13.907280</td>\n    </tr>\n    <tr>\n      <th>243</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>61.5</td>\n      <td>550</td>\n      <td>15.999231</td>\n      <td>14.779844</td>\n      <td>13.749581</td>\n    </tr>\n    <tr>\n      <th>244</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>63.0</td>\n      <td>550</td>\n      <td>15.349117</td>\n      <td>14.221099</td>\n      <td>13.605475</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>64.5</td>\n      <td>550</td>\n      <td>14.683338</td>\n      <td>13.664426</td>\n      <td>13.474963</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>66.0</td>\n      <td>550</td>\n      <td>14.001893</td>\n      <td>13.109825</td>\n      <td>13.358045</td>\n    </tr>\n  </tbody>\n</table>\n<p>247 rows Ã— 19 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "\n",
    "dir_p = r'C:\\Users\\Honeyz\\Desktop\\Aessa\\THE_SIS\\PyroDataProcessed.csv'\n",
    "raw_dataset = pd.read_csv(dir_p, skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset = dataset.dropna()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold(n_splits=247, random_state=None, shuffle=False) \n",
      "\n",
      "loss for fold 1: \n",
      " Regressor 1: 5.366457259999997\n",
      " Regressor 2:0.2916484199999996\n",
      " Regressor 3:2.745240210000002\n",
      "\n",
      "\n",
      "loss for fold 2: \n",
      " Regressor 1: 0.8868719999999968\n",
      " Regressor 2:0.40788\n",
      " Regressor 3:0.9828787200000022\n",
      "\n",
      "\n",
      "loss for fold 3: \n",
      " Regressor 1: 4.601992670000001\n",
      " Regressor 2:0.2587547800000003\n",
      " Regressor 3:4.369553410000002\n",
      "\n",
      "\n",
      "loss for fold 4: \n",
      " Regressor 1: 0.219270029999997\n",
      " Regressor 2:0.38663430000000076\n",
      " Regressor 3:2.4917181900000003\n",
      "\n",
      "\n",
      "loss for fold 5: \n",
      " Regressor 1: 1.0323309300000005\n",
      " Regressor 2:0.7345481000000049\n",
      " Regressor 3:0.2752151500000011\n",
      "\n",
      "\n",
      "loss for fold 6: \n",
      " Regressor 1: 1.6059359999999998\n",
      " Regressor 2:0.8323199999999957\n",
      " Regressor 3:0.06820223999999797\n",
      "\n",
      "\n",
      "loss for fold 7: \n",
      " Regressor 1: 1.6059359999999998\n",
      " Regressor 2:0.8323199999999957\n",
      " Regressor 3:0.06820223999999797\n",
      "\n",
      "\n",
      "loss for fold 8: \n",
      " Regressor 1: 1.5621599999999987\n",
      " Regressor 2:0.8323200000000028\n",
      " Regressor 3:0.13377408000000202\n",
      "\n",
      "\n",
      "loss for fold 9: \n",
      " Regressor 1: 1.518384000000001\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.19353983999999969\n",
      "\n",
      "\n",
      "loss for fold 10: \n",
      " Regressor 1: 1.4746080000000035\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.24749951999999809\n",
      "\n",
      "\n",
      "loss for fold 11: \n",
      " Regressor 1: 1.4308319999999952\n",
      " Regressor 2:0.8323200000000028\n",
      " Regressor 3:0.2956531200000043\n",
      "\n",
      "\n",
      "loss for fold 12: \n",
      " Regressor 1: 1.3870560000000012\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.3380006399999971\n",
      "\n",
      "\n",
      "loss for fold 13: \n",
      " Regressor 1: 1.34328\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.37454207999999767\n",
      "\n",
      "\n",
      "loss for fold 14: \n",
      " Regressor 1: 1.2995039999999989\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.405277439999999\n",
      "\n",
      "\n",
      "loss for fold 15: \n",
      " Regressor 1: 1.2557279999999977\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.43020672000000104\n",
      "\n",
      "\n",
      "loss for fold 16: \n",
      " Regressor 1: 1.1681760000000025\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.46264704000000023\n",
      "\n",
      "\n",
      "loss for fold 17: \n",
      " Regressor 1: 1.1681760000000025\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.46264704000000023\n",
      "\n",
      "\n",
      "loss for fold 18: \n",
      " Regressor 1: 0.8613539700000032\n",
      " Regressor 2:0.4456856999999985\n",
      " Regressor 3:2.9635812300000026\n",
      "\n",
      "\n",
      "loss for fold 19: \n",
      " Regressor 1: 0.219270029999997\n",
      " Regressor 2:0.38663430000000076\n",
      " Regressor 3:2.4917181900000003\n",
      "\n",
      "\n",
      "loss for fold 20: \n",
      " Regressor 1: 1.036847999999999\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.46776192000000094\n",
      "\n",
      "\n",
      "loss for fold 21: \n",
      " Regressor 1: 0.9930720000000051\n",
      " Regressor 2:0.8323200000000028\n",
      " Regressor 3:0.45785472000000027\n",
      "\n",
      "\n",
      "loss for fold 22: \n",
      " Regressor 1: 0.9492959999999968\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.44214144000000033\n",
      "\n",
      "\n",
      "loss for fold 23: \n",
      " Regressor 1: 0.9055200000000028\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.420622079999994\n",
      "\n",
      "\n",
      "loss for fold 24: \n",
      " Regressor 1: 0.8179680000000005\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.3601651199999978\n",
      "\n",
      "\n",
      "loss for fold 25: \n",
      " Regressor 1: 3.827800670000002\n",
      " Regressor 2:0.5735652200000025\n",
      " Regressor 3:4.048325890000001\n",
      "\n",
      "\n",
      "loss for fold 26: \n",
      " Regressor 1: 4.601992670000001\n",
      " Regressor 2:0.2587547800000003\n",
      " Regressor 3:4.369553410000002\n",
      "\n",
      "\n",
      "loss for fold 27: \n",
      " Regressor 1: 0.7304159999999982\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.2764838400000045\n",
      "\n",
      "\n",
      "loss for fold 28: \n",
      " Regressor 1: 0.686639999999997\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.22593407999999826\n",
      "\n",
      "\n",
      "loss for fold 29: \n",
      " Regressor 1: 0.642864000000003\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.16957823999999988\n",
      "\n",
      "\n",
      "loss for fold 30: \n",
      " Regressor 1: 0.5553120000000007\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.03944832000000176\n",
      "\n",
      "\n",
      "loss for fold 31: \n",
      " Regressor 1: 0.5553120000000007\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.03944832000000176\n",
      "\n",
      "\n",
      "loss for fold 32: \n",
      " Regressor 1: 5.8342172599999955\n",
      " Regressor 2:1.1239684199999989\n",
      " Regressor 3:2.6313342900000016\n",
      "\n",
      "\n",
      "loss for fold 33: \n",
      " Regressor 1: 5.366457259999997\n",
      " Regressor 2:0.2916484199999996\n",
      " Regressor 3:2.745240210000002\n",
      "\n",
      "\n",
      "loss for fold 34: \n",
      " Regressor 1: 0.42398399999999725\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.19929215999999883\n",
      "\n",
      "\n",
      "loss for fold 35: \n",
      " Regressor 1: 0.3802080000000032\n",
      " Regressor 2:0.8323200000000011\n",
      " Regressor 3:0.2904844800000035\n",
      "\n",
      "\n",
      "loss for fold 36: \n",
      " Regressor 1: 0.2926560000000009\n",
      " Regressor 2:0.8323200000000002\n",
      " Regressor 3:0.4902873599999964\n",
      "\n",
      "\n",
      "loss for fold 37: \n",
      " Regressor 1: 0.2926560000000009\n",
      " Regressor 2:0.8323200000000002\n",
      " Regressor 3:0.4902873599999964\n",
      "\n",
      "\n",
      "loss for fold 38: \n",
      " Regressor 1: 0.24887999999999977\n",
      " Regressor 2:0.8323200000000002\n",
      " Regressor 3:0.5988979199999989\n",
      "\n",
      "\n",
      "loss for fold 39: \n",
      " Regressor 1: 0.16132799999999747\n",
      " Regressor 2:0.8323200000000002\n",
      " Regressor 3:0.8335372800000016\n",
      "\n",
      "\n",
      "loss for fold 40: \n",
      " Regressor 1: 0.16132799999999747\n",
      " Regressor 2:0.8323200000000002\n",
      " Regressor 3:0.8335372800000016\n",
      "\n",
      "\n",
      "loss for fold 41: \n",
      " Regressor 1: 0.11755199999999633\n",
      " Regressor 2:0.8323199999999993\n",
      " Regressor 3:0.9595660800000019\n",
      "\n",
      "\n",
      "loss for fold 42: \n",
      " Regressor 1: 0.07377600000000228\n",
      " Regressor 2:0.8323200000000006\n",
      " Regressor 3:1.0914009599999943\n",
      "\n",
      "\n",
      "loss for fold 43: \n",
      " Regressor 1: 0.030000000000001137\n",
      " Regressor 2:0.8323199999999997\n",
      " Regressor 3:1.2290419200000002\n",
      "\n",
      "\n",
      "loss for fold 44: \n",
      " Regressor 1: 0.05755200000000116\n",
      " Regressor 2:0.83232\n",
      " Regressor 3:1.5217420799999957\n",
      "\n",
      "\n",
      "loss for fold 45: \n",
      " Regressor 1: 0.05755200000000116\n",
      " Regressor 2:0.83232\n",
      " Regressor 3:1.5217420799999957\n",
      "\n",
      "\n",
      "loss for fold 46: \n",
      " Regressor 1: 0.8868719999999968\n",
      " Regressor 2:0.40788\n",
      " Regressor 3:0.9828787200000022\n",
      "\n",
      "\n",
      "loss for fold 47: \n",
      " Regressor 1: 0.4151159999999976\n",
      " Regressor 2:0.5862960000000008\n",
      " Regressor 3:0.19306759999999912\n",
      "\n",
      "\n",
      "loss for fold 48: \n",
      " Regressor 1: 1.4835640000000012\n",
      " Regressor 2:0.7353839999999998\n",
      " Regressor 3:2.527424400000001\n",
      "\n",
      "\n",
      "loss for fold 49: \n",
      " Regressor 1: 3.562404000000001\n",
      " Regressor 2:0.007463999999998805\n",
      " Regressor 3:3.4484492000000024\n",
      "\n",
      "\n",
      "loss for fold 50: \n",
      " Regressor 1: 3.5404239999999874\n",
      " Regressor 2:0.7777359999999991\n",
      " Regressor 3:3.570729600000001\n",
      "\n",
      "\n",
      "loss for fold 51: \n",
      " Regressor 1: 0.06090000000000373\n",
      " Regressor 2:0.7122000000000002\n",
      " Regressor 3:2.2901499999999997\n",
      "\n",
      "\n",
      "loss for fold 52: \n",
      " Regressor 1: 0.4151159999999976\n",
      " Regressor 2:0.5862960000000008\n",
      " Regressor 3:0.19306759999999912\n",
      "\n",
      "\n",
      "loss for fold 53: \n",
      " Regressor 1: 0.4020280000000014\n",
      " Regressor 2:0.27684799999999754\n",
      " Regressor 3:0.3170067999999979\n",
      "\n",
      "\n",
      "loss for fold 54: \n",
      " Regressor 1: 0.43494000000000455\n",
      " Regressor 2:0.29040000000000177\n",
      " Regressor 3:0.23627559999999903\n",
      "\n",
      "\n",
      "loss for fold 55: \n",
      " Regressor 1: 0.5007640000000038\n",
      " Regressor 2:0.31750399999999956\n",
      " Regressor 3:0.08439639999999926\n",
      "\n",
      "\n",
      "loss for fold 56: \n",
      " Regressor 1: 0.5007640000000038\n",
      " Regressor 2:0.31750399999999956\n",
      " Regressor 3:0.08439639999999926\n",
      "\n",
      "\n",
      "loss for fold 57: \n",
      " Regressor 1: 0.5336759999999998\n",
      " Regressor 2:0.33105600000000024\n",
      " Regressor 3:0.013248400000001936\n",
      "\n",
      "\n",
      "loss for fold 58: \n",
      " Regressor 1: 0.5995000000000061\n",
      " Regressor 2:0.35815999999999804\n",
      " Regressor 3:0.1194644000000018\n",
      "\n",
      "\n",
      "loss for fold 59: \n",
      " Regressor 1: 0.6324119999999951\n",
      " Regressor 2:0.37171200000000226\n",
      " Regressor 3:0.18102919999999756\n",
      "\n",
      "\n",
      "loss for fold 60: \n",
      " Regressor 1: 0.6324119999999951\n",
      " Regressor 2:0.37171200000000226\n",
      " Regressor 3:0.18102919999999756\n",
      "\n",
      "\n",
      "loss for fold 61: \n",
      " Regressor 1: 0.6982359999999943\n",
      " Regressor 2:0.39881600000000006\n",
      " Regressor 3:0.29457559999999816\n",
      "\n",
      "\n",
      "loss for fold 62: \n",
      " Regressor 1: 1.4835640000000012\n",
      " Regressor 2:0.7353839999999998\n",
      " Regressor 3:2.527424400000001\n",
      "\n",
      "\n",
      "loss for fold 63: \n",
      " Regressor 1: 0.7640600000000006\n",
      " Regressor 2:0.42591999999999786\n",
      " Regressor 3:0.3953444000000026\n",
      "\n",
      "\n",
      "loss for fold 64: \n",
      " Regressor 1: 0.7969719999999967\n",
      " Regressor 2:0.4394720000000021\n",
      " Regressor 3:0.4409372000000005\n",
      "\n",
      "\n",
      "loss for fold 65: \n",
      " Regressor 1: 0.8298839999999998\n",
      " Regressor 2:0.4530239999999992\n",
      " Regressor 3:0.4833356000000002\n",
      "\n",
      "\n",
      "loss for fold 66: \n",
      " Regressor 1: 0.8298839999999998\n",
      " Regressor 2:0.4530239999999992\n",
      " Regressor 3:0.4833356000000002\n",
      "\n",
      "\n",
      "loss for fold 67: \n",
      " Regressor 1: 0.8957079999999991\n",
      " Regressor 2:0.48012800000000055\n",
      " Regressor 3:0.5585492000000016\n",
      "\n",
      "\n",
      "loss for fold 68: \n",
      " Regressor 1: 0.8957079999999991\n",
      " Regressor 2:0.48012800000000055\n",
      " Regressor 3:0.5585492000000016\n",
      "\n",
      "\n",
      "loss for fold 69: \n",
      " Regressor 1: 0.9615319999999983\n",
      " Regressor 2:0.5072319999999984\n",
      " Regressor 3:0.6209851999999998\n",
      "\n",
      "\n",
      "loss for fold 70: \n",
      " Regressor 1: 0.9615319999999983\n",
      " Regressor 2:0.5072319999999984\n",
      " Regressor 3:0.6209851999999998\n",
      "\n",
      "\n",
      "loss for fold 71: \n",
      " Regressor 1: 0.9944440000000014\n",
      " Regressor 2:0.520783999999999\n",
      " Regressor 3:0.6474115999999981\n",
      "\n",
      "\n",
      "loss for fold 72: \n",
      " Regressor 1: 1.0602680000000007\n",
      " Regressor 2:0.5478880000000004\n",
      " Regressor 3:0.6906812000000002\n",
      "\n",
      "\n",
      "loss for fold 73: \n",
      " Regressor 1: 2.469224000000004\n",
      " Regressor 2:0.5539760000000022\n",
      " Regressor 3:2.740924800000002\n",
      "\n",
      "\n",
      "loss for fold 74: \n",
      " Regressor 1: 3.562404000000001\n",
      " Regressor 2:0.007463999999998805\n",
      " Regressor 3:3.4484492000000024\n",
      "\n",
      "\n",
      "loss for fold 75: \n",
      " Regressor 1: 1.1260919999999999\n",
      " Regressor 2:0.5749919999999982\n",
      " Regressor 3:0.7211731999999991\n",
      "\n",
      "\n",
      "loss for fold 76: \n",
      " Regressor 1: 1.159003999999996\n",
      " Regressor 2:0.5885440000000024\n",
      " Regressor 3:0.7316275999999995\n",
      "\n",
      "\n",
      "loss for fold 77: \n",
      " Regressor 1: 1.2248280000000022\n",
      " Regressor 2:0.6156480000000002\n",
      " Regressor 3:0.7429532000000023\n",
      "\n",
      "\n",
      "loss for fold 78: \n",
      " Regressor 1: 1.2248280000000022\n",
      " Regressor 2:0.6156480000000002\n",
      " Regressor 3:0.7429532000000023\n",
      "\n",
      "\n",
      "loss for fold 79: \n",
      " Regressor 1: 1.2906519999999944\n",
      " Regressor 2:0.6427519999999998\n",
      " Regressor 3:0.7415012000000001\n",
      "\n",
      "\n",
      "loss for fold 80: \n",
      " Regressor 1: 1.3235640000000046\n",
      " Regressor 2:0.6563040000000004\n",
      " Regressor 3:0.7359835999999991\n",
      "\n",
      "\n",
      "loss for fold 81: \n",
      " Regressor 1: 1.3235640000000046\n",
      " Regressor 2:0.6563040000000004\n",
      " Regressor 3:0.7359835999999991\n",
      "\n",
      "\n",
      "loss for fold 82: \n",
      " Regressor 1: 1.3893879999999967\n",
      " Regressor 2:0.683408\n",
      " Regressor 3:0.7153652000000008\n",
      "\n",
      "\n",
      "loss for fold 83: \n",
      " Regressor 1: 1.3893879999999967\n",
      " Regressor 2:0.683408\n",
      " Regressor 3:0.7153652000000008\n",
      "\n",
      "\n",
      "loss for fold 84: \n",
      " Regressor 1: 1.455212000000003\n",
      " Regressor 2:0.7105119999999996\n",
      " Regressor 3:0.6819691999999993\n",
      "\n",
      "\n",
      "loss for fold 85: \n",
      " Regressor 1: 3.5404239999999874\n",
      " Regressor 2:0.7777359999999991\n",
      " Regressor 3:3.570729600000001\n",
      "\n",
      "\n",
      "loss for fold 86: \n",
      " Regressor 1: 2.0522999999999882\n",
      " Regressor 2:1.5017999999999994\n",
      " Regressor 3:2.9102500000000004\n",
      "\n",
      "\n",
      "loss for fold 87: \n",
      " Regressor 1: 1.5210359999999952\n",
      " Regressor 2:0.7376160000000009\n",
      " Regressor 3:0.6357955999999998\n",
      "\n",
      "\n",
      "loss for fold 88: \n",
      " Regressor 1: 1.5539480000000054\n",
      " Regressor 2:0.7511679999999998\n",
      " Regressor 3:0.607917200000001\n",
      "\n",
      "\n",
      "loss for fold 89: \n",
      " Regressor 1: 1.5868599999999873\n",
      " Regressor 2:0.7647199999999987\n",
      " Regressor 3:0.5768443999999988\n",
      "\n",
      "\n",
      "loss for fold 90: \n",
      " Regressor 1: 1.6197720000000118\n",
      " Regressor 2:0.7782720000000012\n",
      " Regressor 3:0.5425772000000002\n",
      "\n",
      "\n",
      "loss for fold 91: \n",
      " Regressor 1: 1.6526839999999936\n",
      " Regressor 2:0.7918239999999992\n",
      " Regressor 3:0.5051156000000008\n",
      "\n",
      "\n",
      "loss for fold 92: \n",
      " Regressor 1: 1.6855960000000039\n",
      " Regressor 2:0.8053760000000008\n",
      " Regressor 3:0.46445959999999964\n",
      "\n",
      "\n",
      "loss for fold 93: \n",
      " Regressor 1: 1.718508\n",
      " Regressor 2:0.8189279999999997\n",
      " Regressor 3:0.42060920000000035\n",
      "\n",
      "\n",
      "loss for fold 94: \n",
      " Regressor 1: 1.751419999999996\n",
      " Regressor 2:0.8324800000000003\n",
      " Regressor 3:0.37356439999999935\n",
      "\n",
      "\n",
      "loss for fold 95: \n",
      " Regressor 1: 1.817243999999988\n",
      " Regressor 2:0.8595839999999999\n",
      " Regressor 3:0.26989160000000023\n",
      "\n",
      "\n",
      "loss for fold 96: \n",
      " Regressor 1: 0.06090000000000373\n",
      " Regressor 2:0.7122000000000002\n",
      " Regressor 3:2.2901499999999997\n",
      "\n",
      "\n",
      "loss for fold 97: \n",
      " Regressor 1: 0.607776869999995\n",
      " Regressor 2:0.8176387399999996\n",
      " Regressor 3:0.07054534999999973\n",
      "\n",
      "\n",
      "loss for fold 98: \n",
      " Regressor 1: 0.6062504900000008\n",
      " Regressor 2:0.46779923999999795\n",
      " Regressor 3:0.10238183999999961\n",
      "\n",
      "\n",
      "loss for fold 99: \n",
      " Regressor 1: 8.924111269999997\n",
      " Regressor 2:6.244265662\n",
      " Regressor 3:2.1710396099999976\n",
      "\n",
      "\n",
      "loss for fold 100: \n",
      " Regressor 1: 1.2879599400000004\n",
      " Regressor 2:0.2431866300000003\n",
      " Regressor 3:0.3481311600000012\n",
      "\n",
      "\n",
      "loss for fold 101: \n",
      " Regressor 1: 0.7424630400000041\n",
      " Regressor 2:0.8841757200000018\n",
      " Regressor 3:0.07905669999999887\n",
      "\n",
      "\n",
      "loss for fold 102: \n",
      " Regressor 1: 0.7424630400000041\n",
      " Regressor 2:0.8841757200000018\n",
      " Regressor 3:0.07905669999999887\n",
      "\n",
      "\n",
      "loss for fold 103: \n",
      " Regressor 1: 0.6983817600000037\n",
      " Regressor 2:0.849700679999998\n",
      " Regressor 3:0.11597391000000101\n",
      "\n",
      "\n",
      "loss for fold 104: \n",
      " Regressor 1: 0.6763411199999965\n",
      " Regressor 2:0.8324631600000032\n",
      " Regressor 3:0.13295720999999894\n",
      "\n",
      "\n",
      "loss for fold 105: \n",
      " Regressor 1: 0.6763411199999965\n",
      " Regressor 2:0.8324631600000032\n",
      " Regressor 3:0.13295720999999894\n",
      "\n",
      "\n",
      "loss for fold 106: \n",
      " Regressor 1: 0.6322598399999961\n",
      " Regressor 2:0.7979881199999994\n",
      " Regressor 3:0.16397321999999903\n",
      "\n",
      "\n",
      "loss for fold 107: \n",
      " Regressor 1: 0.6322598399999961\n",
      " Regressor 2:0.7979881199999994\n",
      " Regressor 3:0.16397321999999903\n",
      "\n",
      "\n",
      "loss for fold 108: \n",
      " Regressor 1: 0.6102192000000031\n",
      " Regressor 2:0.7807506000000011\n",
      " Regressor 3:0.17800590999999955\n",
      "\n",
      "\n",
      "loss for fold 109: \n",
      " Regressor 1: 0.5881785600000029\n",
      " Regressor 2:0.7635130799999992\n",
      " Regressor 3:0.19105507999999993\n",
      "\n",
      "\n",
      "loss for fold 110: \n",
      " Regressor 1: 0.5661379199999956\n",
      " Regressor 2:0.7462755600000008\n",
      " Regressor 3:0.20312071000000032\n",
      "\n",
      "\n",
      "loss for fold 111: \n",
      " Regressor 1: 0.5440972800000026\n",
      " Regressor 2:0.7290380399999989\n",
      " Regressor 3:0.21420280999999974\n",
      "\n",
      "\n",
      "loss for fold 112: \n",
      " Regressor 1: 0.5220566399999953\n",
      " Regressor 2:0.7118005200000006\n",
      " Regressor 3:0.22430136000000012\n",
      "\n",
      "\n",
      "loss for fold 113: \n",
      " Regressor 1: 0.5000160000000022\n",
      " Regressor 2:0.6945630000000023\n",
      " Regressor 3:0.2334164000000012\n",
      "\n",
      "\n",
      "loss for fold 114: \n",
      " Regressor 1: 0.47797536000000207\n",
      " Regressor 2:0.6773254800000004\n",
      " Regressor 3:0.24154787999999883\n",
      "\n",
      "\n",
      "loss for fold 115: \n",
      " Regressor 1: 0.4559347200000019\n",
      " Regressor 2:0.6600879599999985\n",
      " Regressor 3:0.24869585000000072\n",
      "\n",
      "\n",
      "loss for fold 116: \n",
      " Regressor 1: 0.19439704999999918\n",
      " Regressor 2:0.1578136800000003\n",
      " Regressor 3:0.1576593200000005\n",
      "\n",
      "\n",
      "loss for fold 117: \n",
      " Regressor 1: 0.6062504900000008\n",
      " Regressor 2:0.46779923999999795\n",
      " Regressor 3:0.10238183999999961\n",
      "\n",
      "\n",
      "loss for fold 118: \n",
      " Regressor 1: 0.3898128000000014\n",
      " Regressor 2:0.6083753999999999\n",
      " Regressor 3:0.264238510000002\n",
      "\n",
      "\n",
      "loss for fold 119: \n",
      " Regressor 1: 0.34573152000000107\n",
      " Regressor 2:0.5739003599999997\n",
      " Regressor 3:0.2696826200000011\n",
      "\n",
      "\n",
      "loss for fold 120: \n",
      " Regressor 1: 0.34573152000000107\n",
      " Regressor 2:0.5739003599999997\n",
      " Regressor 3:0.2696826200000011\n",
      "\n",
      "\n",
      "loss for fold 121: \n",
      " Regressor 1: 0.3236908799999938\n",
      " Regressor 2:0.5566628400000013\n",
      " Regressor 3:0.27092937000000106\n",
      "\n",
      "\n",
      "loss for fold 122: \n",
      " Regressor 1: 0.27960960000000057\n",
      " Regressor 2:0.5221878000000011\n",
      " Regressor 3:0.2704722799999999\n",
      "\n",
      "\n",
      "loss for fold 123: \n",
      " Regressor 1: 0.2575689600000004\n",
      " Regressor 2:0.5049502799999992\n",
      " Regressor 3:0.2687684200000007\n",
      "\n",
      "\n",
      "loss for fold 124: \n",
      " Regressor 1: 0.2575689600000004\n",
      " Regressor 2:0.5049502799999992\n",
      " Regressor 3:0.2687684200000007\n",
      "\n",
      "\n",
      "loss for fold 125: \n",
      " Regressor 1: 0.21348768000000007\n",
      " Regressor 2:0.47047523999999896\n",
      " Regressor 3:0.26241012000000197\n",
      "\n",
      "\n",
      "loss for fold 126: \n",
      " Regressor 1: 0.21348768000000007\n",
      " Regressor 2:0.47047523999999896\n",
      " Regressor 3:0.26241012000000197\n",
      "\n",
      "\n",
      "loss for fold 127: \n",
      " Regressor 1: 0.1914470399999999\n",
      " Regressor 2:0.4532377200000006\n",
      " Regressor 3:0.2577556599999973\n",
      "\n",
      "\n",
      "loss for fold 128: \n",
      " Regressor 1: 0.14736575999999957\n",
      " Regressor 2:0.4187626800000004\n",
      " Regressor 3:0.24549615000000102\n",
      "\n",
      "\n",
      "loss for fold 129: \n",
      " Regressor 1: 0.1253251199999994\n",
      " Regressor 2:0.4015251599999985\n",
      " Regressor 3:0.23789108999999797\n",
      "\n",
      "\n",
      "loss for fold 130: \n",
      " Regressor 1: 0.1253251199999994\n",
      " Regressor 2:0.4015251599999985\n",
      " Regressor 3:0.23789108999999797\n",
      "\n",
      "\n",
      "loss for fold 131: \n",
      " Regressor 1: 0.10328448000000634\n",
      " Regressor 2:0.38428764000000015\n",
      " Regressor 3:0.2293024999999993\n",
      "\n",
      "\n",
      "loss for fold 132: \n",
      " Regressor 1: 0.08124383999999907\n",
      " Regressor 2:0.3670501200000018\n",
      " Regressor 3:0.21973038000000145\n",
      "\n",
      "\n",
      "loss for fold 133: \n",
      " Regressor 1: 6.434306153333324\n",
      " Regressor 2:4.533050224666665\n",
      " Regressor 3:1.3998166999999953\n",
      "\n",
      "\n",
      "loss for fold 134: \n",
      " Regressor 1: 6.397143593333325\n",
      " Regressor 2:4.200475144666665\n",
      " Regressor 3:1.597452220000001\n",
      "\n",
      "\n",
      "loss for fold 135: \n",
      " Regressor 1: 0.015121919999998568\n",
      " Regressor 2:0.3153375599999997\n",
      " Regressor 3:0.18511278999999803\n",
      "\n",
      "\n",
      "loss for fold 136: \n",
      " Regressor 1: 0.006918720000001599\n",
      " Regressor 2:0.29810003999999957\n",
      " Regressor 3:0.1716065300000018\n",
      "\n",
      "\n",
      "loss for fold 137: \n",
      " Regressor 1: 0.02895935999999466\n",
      " Regressor 2:0.2808625200000012\n",
      " Regressor 3:0.15711671999999766\n",
      "\n",
      "\n",
      "loss for fold 138: \n",
      " Regressor 1: 0.05100000000000193\n",
      " Regressor 2:0.26362499999999933\n",
      " Regressor 3:0.14164339999999953\n",
      "\n",
      "\n",
      "loss for fold 139: \n",
      " Regressor 1: 0.09508127999999516\n",
      " Regressor 2:0.22914996000000087\n",
      " Regressor 3:0.10774612999999889\n",
      "\n",
      "\n",
      "loss for fold 140: \n",
      " Regressor 1: 0.09508127999999516\n",
      " Regressor 2:0.22914996000000087\n",
      " Regressor 3:0.10774612999999889\n",
      "\n",
      "\n",
      "loss for fold 141: \n",
      " Regressor 1: 0.11712192000000243\n",
      " Regressor 2:0.21191243999999898\n",
      " Regressor 3:0.08932219000000075\n",
      "\n",
      "\n",
      "loss for fold 142: \n",
      " Regressor 1: 0.1391625600000026\n",
      " Regressor 2:0.19467492000000064\n",
      " Regressor 3:0.06991471999999987\n",
      "\n",
      "\n",
      "loss for fold 143: \n",
      " Regressor 1: 0.18324384000000293\n",
      " Regressor 2:0.1601998800000004\n",
      " Regressor 3:0.028149179999999774\n",
      "\n",
      "\n",
      "loss for fold 144: \n",
      " Regressor 1: 0.2052844800000031\n",
      " Regressor 2:0.1429623599999985\n",
      " Regressor 3:0.005791099999999716\n",
      "\n",
      "\n",
      "loss for fold 145: \n",
      " Regressor 1: 0.2052844800000031\n",
      " Regressor 2:0.1429623599999985\n",
      " Regressor 3:0.005791099999999716\n",
      "\n",
      "\n",
      "loss for fold 146: \n",
      " Regressor 1: 0.22732511999999616\n",
      " Regressor 2:0.12572484000000017\n",
      " Regressor 3:0.01755050999999952\n",
      "\n",
      "\n",
      "loss for fold 147: \n",
      " Regressor 1: 1.8273385199999979\n",
      " Regressor 2:0.39733632999999813\n",
      " Regressor 3:1.1925320400000032\n",
      "\n",
      "\n",
      "loss for fold 148: \n",
      " Regressor 1: 1.1594005500000009\n",
      " Regressor 2:0.36862997000000064\n",
      " Regressor 3:0.08026539999999827\n",
      "\n",
      "\n",
      "loss for fold 149: \n",
      " Regressor 1: 0.6258740500000073\n",
      " Regressor 2:1.0495294200000007\n",
      " Regressor 3:0.10830857999999921\n",
      "\n",
      "\n",
      "loss for fold 150: \n",
      " Regressor 1: 4.35263578\n",
      " Regressor 2:3.723876538999999\n",
      " Regressor 3:1.0456104750000002\n",
      "\n",
      "\n",
      "loss for fold 151: \n",
      " Regressor 1: 0.9769390400000049\n",
      " Regressor 2:1.176141\n",
      " Regressor 3:0.3436827799999982\n",
      "\n",
      "\n",
      "loss for fold 152: \n",
      " Regressor 1: 1.8273385199999979\n",
      " Regressor 2:0.39733632999999813\n",
      " Regressor 3:1.1925320400000032\n",
      "\n",
      "\n",
      "loss for fold 153: \n",
      " Regressor 1: 1.3142983999999984\n",
      " Regressor 2:0.41216512000000094\n",
      " Regressor 3:1.1563306699999956\n",
      "\n",
      "\n",
      "loss for fold 154: \n",
      " Regressor 1: 1.293844\n",
      " Regressor 2:0.41455520000000234\n",
      " Regressor 3:1.0996190200000058\n",
      "\n",
      "\n",
      "loss for fold 155: \n",
      " Regressor 1: 1.2733896000000016\n",
      " Regressor 2:0.41694527999999664\n",
      " Regressor 3:1.0444247499999975\n",
      "\n",
      "\n",
      "loss for fold 156: \n",
      " Regressor 1: 1.2324807999999976\n",
      " Regressor 2:0.42172543999999945\n",
      " Regressor 3:0.938588339999999\n",
      "\n",
      "\n",
      "loss for fold 157: \n",
      " Regressor 1: 1.2324807999999976\n",
      " Regressor 2:0.42172543999999945\n",
      " Regressor 3:0.938588339999999\n",
      "\n",
      "\n",
      "loss for fold 158: \n",
      " Regressor 1: 1.2120263999999992\n",
      " Regressor 2:0.42411552000000086\n",
      " Regressor 3:0.887946190000001\n",
      "\n",
      "\n",
      "loss for fold 159: \n",
      " Regressor 1: 1.1915720000000007\n",
      " Regressor 2:0.4265055999999987\n",
      " Regressor 3:0.8388214199999986\n",
      "\n",
      "\n",
      "loss for fold 160: \n",
      " Regressor 1: 1.1711176000000023\n",
      " Regressor 2:0.4288956800000001\n",
      " Regressor 3:0.7912140300000026\n",
      "\n",
      "\n",
      "loss for fold 161: \n",
      " Regressor 1: 1.1594005500000009\n",
      " Regressor 2:0.36862997000000064\n",
      " Regressor 3:0.08026539999999827\n",
      "\n",
      "\n",
      "loss for fold 162: \n",
      " Regressor 1: 1.1097544\n",
      " Regressor 2:0.4360659200000008\n",
      " Regressor 3:0.6574961100000003\n",
      "\n",
      "\n",
      "loss for fold 163: \n",
      " Regressor 1: 1.1097544\n",
      " Regressor 2:0.4360659200000008\n",
      " Regressor 3:0.6574961100000003\n",
      "\n",
      "\n",
      "loss for fold 164: \n",
      " Regressor 1: 1.0893000000000015\n",
      " Regressor 2:0.4384559999999986\n",
      " Regressor 3:0.6159582199999996\n",
      "\n",
      "\n",
      "loss for fold 165: \n",
      " Regressor 1: 1.068845600000003\n",
      " Regressor 2:0.44084608000000003\n",
      " Regressor 3:0.5759377100000016\n",
      "\n",
      "\n",
      "loss for fold 166: \n",
      " Regressor 1: 1.0483911999999975\n",
      " Regressor 2:0.44323616000000143\n",
      " Regressor 3:0.5374345799999993\n",
      "\n",
      "\n",
      "loss for fold 167: \n",
      " Regressor 1: 1.027936799999999\n",
      " Regressor 2:0.4456262399999993\n",
      " Regressor 3:0.500448819999999\n",
      "\n",
      "\n",
      "loss for fold 168: \n",
      " Regressor 1: 1.0074824000000007\n",
      " Regressor 2:0.4480163200000007\n",
      " Regressor 3:0.46498043000000067\n",
      "\n",
      "\n",
      "loss for fold 169: \n",
      " Regressor 1: 0.9665735999999967\n",
      " Regressor 2:0.45279647999999995\n",
      " Regressor 3:0.39859579000000167\n",
      "\n",
      "\n",
      "loss for fold 170: \n",
      " Regressor 1: 0.9665735999999967\n",
      " Regressor 2:0.45279647999999995\n",
      " Regressor 3:0.39859579000000167\n",
      "\n",
      "\n",
      "loss for fold 171: \n",
      " Regressor 1: 0.6258740500000073\n",
      " Regressor 2:1.0495294200000007\n",
      " Regressor 3:0.10830857999999921\n",
      "\n",
      "\n",
      "loss for fold 172: \n",
      " Regressor 1: 0.9052104000000014\n",
      " Regressor 2:0.4599667200000006\n",
      " Regressor 3:0.3103991499999985\n",
      "\n",
      "\n",
      "loss for fold 173: \n",
      " Regressor 1: 0.9052104000000014\n",
      " Regressor 2:0.4599667200000006\n",
      " Regressor 3:0.3103991499999985\n",
      "\n",
      "\n",
      "loss for fold 174: \n",
      " Regressor 1: 0.8643016000000046\n",
      " Regressor 2:0.46474687999999986\n",
      " Regressor 3:0.2591882699999992\n",
      "\n",
      "\n",
      "loss for fold 175: \n",
      " Regressor 1: 0.843847199999999\n",
      " Regressor 2:0.4671369599999995\n",
      " Regressor 3:0.2358589000000002\n",
      "\n",
      "\n",
      "loss for fold 176: \n",
      " Regressor 1: 0.843847199999999\n",
      " Regressor 2:0.4671369599999995\n",
      " Regressor 3:0.2358589000000002\n",
      "\n",
      "\n",
      "loss for fold 177: \n",
      " Regressor 1: 0.8233928000000077\n",
      " Regressor 2:0.4695270399999991\n",
      " Regressor 3:0.2140468999999996\n",
      "\n",
      "\n",
      "loss for fold 178: \n",
      " Regressor 1: 0.802938399999988\n",
      " Regressor 2:0.4719171200000005\n",
      " Regressor 3:0.19375227000000095\n",
      "\n",
      "\n",
      "loss for fold 179: \n",
      " Regressor 1: 0.7824840000000108\n",
      " Regressor 2:0.47430720000000015\n",
      " Regressor 3:0.17497502000000154\n",
      "\n",
      "\n",
      "loss for fold 180: \n",
      " Regressor 1: 0.7620295999999911\n",
      " Regressor 2:0.4766972799999998\n",
      " Regressor 3:0.1577151499999978\n",
      "\n",
      "\n",
      "loss for fold 181: \n",
      " Regressor 1: 4.571475489999997\n",
      " Regressor 2:2.520389123000001\n",
      " Regressor 3:1.125429135000001\n",
      "\n",
      "\n",
      "loss for fold 182: \n",
      " Regressor 1: 3.768771126666678\n",
      " Regressor 2:2.1617368553333343\n",
      " Regressor 3:0.8780336300000045\n",
      "\n",
      "\n",
      "loss for fold 183: \n",
      " Regressor 1: 0.6802119999999974\n",
      " Regressor 2:0.48625760000000007\n",
      " Regressor 3:0.10384941999999953\n",
      "\n",
      "\n",
      "loss for fold 184: \n",
      " Regressor 1: 0.6802119999999974\n",
      " Regressor 2:0.48625760000000007\n",
      " Regressor 3:0.10384941999999953\n",
      "\n",
      "\n",
      "loss for fold 185: \n",
      " Regressor 1: 0.6597575999999918\n",
      " Regressor 2:0.4886476799999997\n",
      " Regressor 3:0.09417642999999742\n",
      "\n",
      "\n",
      "loss for fold 186: \n",
      " Regressor 1: 0.6393032000000005\n",
      " Regressor 2:0.4910377600000002\n",
      " Regressor 3:0.08602082000000166\n",
      "\n",
      "\n",
      "loss for fold 187: \n",
      " Regressor 1: 0.6188488000000092\n",
      " Regressor 2:0.49342783999999984\n",
      " Regressor 3:0.07938258000000076\n",
      "\n",
      "\n",
      "loss for fold 188: \n",
      " Regressor 1: 0.5983943999999894\n",
      " Regressor 2:0.49581792000000036\n",
      " Regressor 3:0.07426170999999826\n",
      "\n",
      "\n",
      "loss for fold 189: \n",
      " Regressor 1: 0.5574856000000068\n",
      " Regressor 2:0.5005980799999996\n",
      " Regressor 3:0.06857210999999808\n",
      "\n",
      "\n",
      "loss for fold 190: \n",
      " Regressor 1: 0.5574856000000068\n",
      " Regressor 2:0.5005980799999996\n",
      " Regressor 3:0.06857210999999808\n",
      "\n",
      "\n",
      "loss for fold 191: \n",
      " Regressor 1: 0.5370312000000013\n",
      " Regressor 2:0.5029881600000001\n",
      " Regressor 3:0.06800338000000039\n",
      "\n",
      "\n",
      "loss for fold 192: \n",
      " Regressor 1: 0.4961224000000044\n",
      " Regressor 2:0.5077683200000003\n",
      " Regressor 3:0.07141803000000024\n",
      "\n",
      "\n",
      "loss for fold 193: \n",
      " Regressor 1: 0.47566799999999887\n",
      " Regressor 2:0.5101583999999999\n",
      " Regressor 3:0.07540141999999861\n",
      "\n",
      "\n",
      "loss for fold 194: \n",
      " Regressor 1: 0.45521359999999333\n",
      " Regressor 2:0.51254848\n",
      " Regressor 3:0.08090218999999976\n",
      "\n",
      "\n",
      "loss for fold 195: \n",
      " Regressor 1: 0.45521359999999333\n",
      " Regressor 2:0.51254848\n",
      " Regressor 3:0.08090218999999976\n",
      "\n",
      "\n",
      "loss for fold 196: \n",
      " Regressor 1: 0.434759200000002\n",
      " Regressor 2:0.51493856\n",
      " Regressor 3:0.08792034000000015\n",
      "\n",
      "\n",
      "loss for fold 197: \n",
      " Regressor 1: 0.9769390400000049\n",
      " Regressor 2:1.176141\n",
      " Regressor 3:0.3436827799999982\n",
      "\n",
      "\n",
      "loss for fold 198: \n",
      " Regressor 1: 0.2885464000000013\n",
      " Regressor 2:2.0183186499999977\n",
      " Regressor 3:0.9539099399999991\n",
      "\n",
      "\n",
      "loss for fold 199: \n",
      " Regressor 1: 70.53418966999999\n",
      " Regressor 2:12.078896765\n",
      " Regressor 3:1.0749448899999994\n",
      "\n",
      "\n",
      "loss for fold 200: \n",
      " Regressor 1: 5.0846328699999965\n",
      " Regressor 2:1.7504103499999957\n",
      " Regressor 3:0.7246434900000018\n",
      "\n",
      "\n",
      "loss for fold 201: \n",
      " Regressor 1: 7.523534780000002\n",
      " Regressor 2:0.24957806999999832\n",
      " Regressor 3:1.6975776600000003\n",
      "\n",
      "\n",
      "loss for fold 202: \n",
      " Regressor 1: 18.626974590000003\n",
      " Regressor 2:0.5140446599999997\n",
      " Regressor 3:0.5244959199999997\n",
      "\n",
      "\n",
      "loss for fold 203: \n",
      " Regressor 1: 36.23951524\n",
      " Regressor 2:0.6911877000000004\n",
      " Regressor 3:0.40083592000000223\n",
      "\n",
      "\n",
      "loss for fold 204: \n",
      " Regressor 1: 0.023498099999997635\n",
      " Regressor 2:0.6416076399999966\n",
      " Regressor 3:0.6878495999999998\n",
      "\n",
      "\n",
      "loss for fold 205: \n",
      " Regressor 1: 0.039163500000000795\n",
      " Regressor 2:0.6395360600000046\n",
      " Regressor 3:0.6742559999999997\n",
      "\n",
      "\n",
      "loss for fold 206: \n",
      " Regressor 1: 5.0846328699999965\n",
      " Regressor 2:1.7504103499999957\n",
      " Regressor 3:0.7246434900000018\n",
      "\n",
      "\n",
      "loss for fold 207: \n",
      " Regressor 1: 5.139461769999997\n",
      " Regressor 2:1.1129458599999964\n",
      " Regressor 3:1.3853058900000015\n",
      "\n",
      "\n",
      "loss for fold 208: \n",
      " Regressor 1: 0.07049430000000001\n",
      " Regressor 2:0.6353929100000002\n",
      " Regressor 3:0.6470687999999996\n",
      "\n",
      "\n",
      "loss for fold 209: \n",
      " Regressor 1: 7.421709680000003\n",
      " Regressor 2:0.38167168999999745\n",
      " Regressor 3:1.077696060000001\n",
      "\n",
      "\n",
      "loss for fold 210: \n",
      " Regressor 1: 7.523534780000002\n",
      " Regressor 2:0.24957806999999832\n",
      " Regressor 3:1.6975776600000003\n",
      "\n",
      "\n",
      "loss for fold 211: \n",
      " Regressor 1: 0.11749050000000238\n",
      " Regressor 2:0.6291781899999975\n",
      " Regressor 3:0.6062879999999993\n",
      "\n",
      "\n",
      "loss for fold 212: \n",
      " Regressor 1: 0.13315589999999844\n",
      " Regressor 2:0.6271066100000056\n",
      " Regressor 3:0.5926944000000027\n",
      "\n",
      "\n",
      "loss for fold 213: \n",
      " Regressor 1: 0.1488213000000016\n",
      " Regressor 2:0.6250350400000002\n",
      " Regressor 3:0.5791007999999991\n",
      "\n",
      "\n",
      "loss for fold 214: \n",
      " Regressor 1: 0.16448669999999765\n",
      " Regressor 2:0.6229634600000011\n",
      " Regressor 3:0.565507199999999\n",
      "\n",
      "\n",
      "loss for fold 215: \n",
      " Regressor 1: 0.18015210000000081\n",
      " Regressor 2:0.6208918899999958\n",
      " Regressor 3:0.5519136000000024\n",
      "\n",
      "\n",
      "loss for fold 216: \n",
      " Regressor 1: 18.626974590000003\n",
      " Regressor 2:0.5140446599999997\n",
      " Regressor 3:0.5244959199999997\n",
      "\n",
      "\n",
      "loss for fold 217: \n",
      " Regressor 1: 18.838457490000003\n",
      " Regressor 2:0.1027040800000023\n",
      " Regressor 3:0.00023047999999903368\n",
      "\n",
      "\n",
      "loss for fold 218: \n",
      " Regressor 1: 0.22714829999999964\n",
      " Regressor 2:0.6146771599999994\n",
      " Regressor 3:0.5111328000000022\n",
      "\n",
      "\n",
      "loss for fold 219: \n",
      " Regressor 1: 0.24281369999999924\n",
      " Regressor 2:0.6126055899999976\n",
      " Regressor 3:0.4975391999999985\n",
      "\n",
      "\n",
      "loss for fold 220: \n",
      " Regressor 1: 0.2584791000000024\n",
      " Regressor 2:0.6105340100000021\n",
      " Regressor 3:0.4839455999999984\n",
      "\n",
      "\n",
      "loss for fold 221: \n",
      " Regressor 1: 0.27414449999999846\n",
      " Regressor 2:0.6084624400000003\n",
      " Regressor 3:0.4703520000000019\n",
      "\n",
      "\n",
      "loss for fold 222: \n",
      " Regressor 1: 0.2898099000000016\n",
      " Regressor 2:0.6063908599999976\n",
      " Regressor 3:0.45675839999999823\n",
      "\n",
      "\n",
      "loss for fold 223: \n",
      " Regressor 1: 0.3054752999999977\n",
      " Regressor 2:0.604319290000003\n",
      " Regressor 3:0.4431648000000017\n",
      "\n",
      "\n",
      "loss for fold 224: \n",
      " Regressor 1: 0.32114070000000083\n",
      " Regressor 2:0.6022477099999968\n",
      " Regressor 3:0.4295712000000016\n",
      "\n",
      "\n",
      "loss for fold 225: \n",
      " Regressor 1: 0.33680610000000044\n",
      " Regressor 2:0.6001761400000021\n",
      " Regressor 3:0.41597759999999795\n",
      "\n",
      "\n",
      "loss for fold 226: \n",
      " Regressor 1: 0.35247150000000005\n",
      " Regressor 2:0.5981045599999995\n",
      " Regressor 3:0.4023840000000014\n",
      "\n",
      "\n",
      "loss for fold 227: \n",
      " Regressor 1: 0.36813689999999966\n",
      " Regressor 2:0.5960329900000012\n",
      " Regressor 3:0.38879039999999776\n",
      "\n",
      "\n",
      "loss for fold 228: \n",
      " Regressor 1: 0.39946769999999887\n",
      " Regressor 2:0.5918898400000003\n",
      " Regressor 3:0.3616032000000011\n",
      "\n",
      "\n",
      "loss for fold 229: \n",
      " Regressor 1: 35.82438214\n",
      " Regressor 2:0.10136943999999914\n",
      " Regressor 3:0.7488455199999997\n",
      "\n",
      "\n",
      "loss for fold 230: \n",
      " Regressor 1: 36.23951524\n",
      " Regressor 2:0.6911877000000004\n",
      " Regressor 3:0.40083592000000223\n",
      "\n",
      "\n",
      "loss for fold 231: \n",
      " Regressor 1: 0.4307984999999981\n",
      " Regressor 2:0.5877466899999995\n",
      " Regressor 3:0.33441600000000093\n",
      "\n",
      "\n",
      "loss for fold 232: \n",
      " Regressor 1: 0.44646390000000125\n",
      " Regressor 2:0.5856751100000004\n",
      " Regressor 3:0.32082240000000084\n",
      "\n",
      "\n",
      "loss for fold 233: \n",
      " Regressor 1: 0.46212930000000085\n",
      " Regressor 2:0.5836035399999986\n",
      " Regressor 3:0.30722880000000075\n",
      "\n",
      "\n",
      "loss for fold 234: \n",
      " Regressor 1: 0.49346010000000007\n",
      " Regressor 2:0.5794603900000013\n",
      " Regressor 3:0.28004160000000056\n",
      "\n",
      "\n",
      "loss for fold 235: \n",
      " Regressor 1: 0.49346010000000007\n",
      " Regressor 2:0.5794603900000013\n",
      " Regressor 3:0.28004160000000056\n",
      "\n",
      "\n",
      "loss for fold 236: \n",
      " Regressor 1: 0.5091254999999997\n",
      " Regressor 2:0.5773888099999986\n",
      " Regressor 3:0.2664479999999987\n",
      "\n",
      "\n",
      "loss for fold 237: \n",
      " Regressor 1: 0.5404562999999989\n",
      " Regressor 2:0.5732456600000013\n",
      " Regressor 3:0.23926080000000027\n",
      "\n",
      "\n",
      "loss for fold 238: \n",
      " Regressor 1: 0.5404562999999989\n",
      " Regressor 2:0.5732456600000013\n",
      " Regressor 3:0.23926080000000027\n",
      "\n",
      "\n",
      "loss for fold 239: \n",
      " Regressor 1: 0.5717870999999981\n",
      " Regressor 2:0.5691025100000004\n",
      " Regressor 3:0.21207360000000008\n",
      "\n",
      "\n",
      "loss for fold 240: \n",
      " Regressor 1: 0.5717870999999981\n",
      " Regressor 2:0.5691025100000004\n",
      " Regressor 3:0.21207360000000008\n",
      "\n",
      "\n",
      "loss for fold 241: \n",
      " Regressor 1: 0.5874525000000013\n",
      " Regressor 2:0.5670309399999987\n",
      " Regressor 3:0.19848\n",
      "\n",
      "\n",
      "loss for fold 242: \n",
      " Regressor 1: 0.6031179000000009\n",
      " Regressor 2:0.5649593600000014\n",
      " Regressor 3:0.1848863999999999\n",
      "\n",
      "\n",
      "loss for fold 243: \n",
      " Regressor 1: 0.6187833000000005\n",
      " Regressor 2:0.5628877899999996\n",
      " Regressor 3:0.1712927999999998\n",
      "\n",
      "\n",
      "loss for fold 244: \n",
      " Regressor 1: 0.6344486999999983\n",
      " Regressor 2:0.5608162100000005\n",
      " Regressor 3:0.1576991999999997\n",
      "\n",
      "\n",
      "loss for fold 245: \n",
      " Regressor 1: 0.6501140999999997\n",
      " Regressor 2:0.5587446399999987\n",
      " Regressor 3:0.1441055999999996\n",
      "\n",
      "\n",
      "loss for fold 246: \n",
      " Regressor 1: 0.6657795000000011\n",
      " Regressor 2:0.5566730600000014\n",
      " Regressor 3:0.1305120000000013\n",
      "\n",
      "\n",
      "loss for fold 247: \n",
      " Regressor 1: 0.6814448999999989\n",
      " Regressor 2:0.5546014899999996\n",
      " Regressor 3:0.11691839999999942\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The following script divides data into attributes and labels:\n",
    "X = dataset.iloc[:, :16].values\n",
    "y = dataset.iloc[:, 16:].values\n",
    "\n",
    "\n",
    "loss_per_fold_0 = []\n",
    "loss_per_fold_1 = []\n",
    "loss_per_fold_2 = []\n",
    "\n",
    "\n",
    "#Divide the data into training and testing sets for KFold Cross validaton loop\n",
    "#Define the K-fold Cross Validator\n",
    "cv = KFold(n_splits=247, shuffle=False)\n",
    "cv.get_n_splits(X)\n",
    "print(cv,\"\\n\")\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    # define model\n",
    "    model = DecisionTreeRegressor()\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    \n",
    "    print(f'loss for fold {fold_no}: \\n Regressor 1: {metrics.mean_absolute_error(y_test[:,0], y_pred[:,0])}\\n Regressor 2:{metrics.mean_absolute_error(y_test[:,1], y_pred[:,1])}\\n Regressor 3:{metrics.mean_absolute_error(y_test[:,2], y_pred[:,2])}\\n\\n')\n",
    "\n",
    "    loss_per_fold_0.append(metrics.mean_absolute_error(y_test[:,0], y_pred[:,0]))\n",
    "    loss_per_fold_1.append(metrics.mean_absolute_error(y_test[:,1], y_pred[:,1]))\n",
    "    loss_per_fold_2.append(metrics.mean_absolute_error(y_test[:,2], y_pred[:,2]))\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average scores for all folds:\n> Mean loss fisrt regressor: 2.0538084307422397\n> Mean loss Second regressor: 0.7211326798124155\n> Mean loss Third regressor: 0.6361512570850203\n------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Average scores for all folds:')\n",
    "print(f'> Mean loss fir st regressor: {np.mean(loss_per_fold_0)}')\n",
    "print(f'> Mean loss Second regressor: {np.mean(loss_per_fold_1)}')\n",
    "print(f'> Mean loss Third regressor: {np.mean(loss_per_fold_2)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ]
}